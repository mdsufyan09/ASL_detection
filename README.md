American Sign Language (ASL) Detection)
ğŸ¯ Goal
Built a deep-learning model that recognizes ASL hand signs (Aâ€“Z + SPACE, DELETE, NOTHING) from images.
Used MobileNetV2 to keep it lightweight and fast enough to train on my CPU.
ğŸ“‚ Dataset
From Kaggle â€“ ASL Alphabet Dataset
29 total classes (Aâ€“Z + 3 extra signs)
Around 3K images per class in the training folder
âš™ï¸ Model Details
Base Model: MobileNetV2 (transfer learning)
Image Size: 64Ã—64
Epochs: 5 (CPU-friendly)
Optimizer: Adam
ğŸ“Š Results
Metric	Score
Training Accuracy	~91%
Validation Accuracy	~64%
ğŸ‘‰ Shows a bit of overfitting â€” the model learns training data really well but struggles a bit on unseen data.
Totally expected since I trained only 5 epochs on a normal CPU.
With a stronger setup (GPU or more time), accuracy can easily go 85â€“95%.
ğŸ§  Quick ML Talk
Underfitting: Model didnâ€™t learn enough â†’ both accuracies low.
Overfitting: Model learned too much from training â†’ big accuracy gap.
Mineâ€™s slightly overfitted â€” small system, less training time. Still performs well overall.
ğŸš€ Future Plans
Train longer on GPU.
Add early stopping & data augmentation.
Fine-tune more layers of MobileNetV2.
ğŸ’» App
Made a simple Streamlit app (app.py) â€” just upload an ASL image and it predicts the alphabet.
Run it with:
streamlit run app.py
ğŸ’¬ Final Note
The project works solidly as a prototype.
It proves that even on limited hardware, you can build an ASL detection system that performs decently â€” and itâ€™ll only get better with more power and training time.